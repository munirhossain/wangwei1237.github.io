[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Large Language Model in Action",
    "section": "",
    "text": "序言\n这是一本关于 LLMs 的书籍。"
  },
  {
    "objectID": "embedding.html#获取-embedding",
    "href": "embedding.html#获取-embedding",
    "title": "1  Embedding",
    "section": "1.1 获取 Embedding",
    "text": "1.1 获取 Embedding\n可以根据 Embedding-V1 API 文档 的介绍，来获取基于百度文心大模型的字符串 Embedding。\n还可以使用 列表 7.5 的方式来获取相同的基于文心大模型的 Embedding。\nembeddings = QianfanEmbeddingsEndpoint()\nquery_result = embeddings.embed_query(\"你是谁？\")\n[0.02949424833059311, -0.054236963391304016, -0.01735987327992916, \n 0.06794580817222595, -0.00020318820315878838, 0.04264984279870987, \n -0.0661700889468193, ……\n ……]"
  },
  {
    "objectID": "embedding.html#可视化",
    "href": "embedding.html#可视化",
    "title": "1  Embedding",
    "section": "1.2 可视化",
    "text": "1.2 可视化\nEmbedding 一般是一种高维数据，为了将这种高维数据可视化，我们可以使用 t-SNE [1] 算法将数据进行降维，然后再做可视化处理。\n利用 列表 7.9 对文档进行向量化，然后将向量数据存储于 Milvus 向量数据库中（默认采用的 Collection 为 LangChainCollection）。\n可以通过 Milvus 提供的 HTTP API 来查看指定的 Collection 的结构：\nhttp://{{MILVUS_URI}}/v1/vector/collections/describe?collectionName=LangChainCollection\n\n\n\n图 1.1: Milvus向量数据的结构\n\n\n\n列表 1.1: 向量数据可视化\n#encoding: utf-8\n\n\"\"\"\n@discribe: demo for the embedding visualization.\n@author: wangwei1237@gmail.com\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pymilvus import connections\nfrom pymilvus import Collection\nfrom sklearn.manifold import TSNE\n\nconnections.connect(\n  host='127.0.0.1',\n  port='8081'\n1)\n\n2collection = Collection(\"LangChainCollection\")\n\nres = collection.query(\n  expr = \"pk &gt;= 0\",\n  offset = 0,\n  limit = 500, \n  output_fields = [\"vector\", \"text\", \"source\", \"title\"],\n3)\n\n4vector_list = [i[\"vector\"] for i in res]\n\n5matrix = np.array(vector_list)\n\ntsne = TSNE(n_components=2, perplexity=15, random_state=42, init='random', learning_rate=200)\n6vis_dims = tsne.fit_transform(matrix)\n\n7plt.scatter(vis_dims[:, 0], vis_dims[:, 1])\nplt.title(\"embedding visualized using t-SNE\")\nplt.show()\n\n\n1\n\n初始化 Milvus 链接\n\n2\n\n选择 LangChainCollection\n\n3\n\n从 LangChainCollection 中检索特定数据\n\n4\n\n只提取结果中的 vector 字段，并生成新的列表\n\n5\n\n将 python 列表转换成矩阵\n\n6\n\n对向量数据进行降维\n\n7\n\n对低维数据进行可视化\n\n\n结果如 图 1.2 所示：\n\n\n\n图 1.2: 向量数据可视化结果\n\n\n\n\n\n\n[1] Introduction to t-SNE: 2023. https://www.datacamp.com/tutorial/introduction-t-sne."
  },
  {
    "objectID": "hallucination.html",
    "href": "hallucination.html",
    "title": "2  幻觉",
    "section": "",
    "text": "如其他技术一样，即便当前 LLM 在各个领域中有着惊人的表现，但是 LLM 也存在着缺陷和局限。而 “幻觉（Hallucination）”就是一种非常常见的缺陷。\n\n\n\n\n\n\n幻觉\n\n\n\n幻觉是自然语言生成领域的一个术语，是指模型生成了看似合理但实际上并不存在的内容。这些内容可能包含虚构的信息、存在前后矛盾的逻辑、甚至是毫无意义的内容。\n幻觉原本是心理学领域的专有名词，用于描述一种特殊类型的知觉体验——在没有外部刺激的情况下，清醒的个体的虚假感觉。\n幻觉是一种不真实的、却又非分真实的虚幻感知。模型容易生成流畅但缺乏真实性的内容，这种现象与心理学中的幻觉极为相似，因此在 LLM 领域，我们把 LLM 的这种缺陷称之为 幻觉。\n\n\n幻觉会严重影响依赖 LLM 的下游业务的表现，导致这些业务在真实场景中无法满足用户需求。大语言模型生成内容的真实性是生成式模型接下来面临的重要科学问题之一。\n幻觉分为两类：\n\n内在幻觉（Intrinsic Hallucinations）：生成的内容与输入的源信息冲突。\n\n\n\n图 2.1: 内在幻觉的例子\n\n\n外在幻觉（Extrinsic Hallucinations）：生成了与源信息无关的内容。外在幻觉可能与事实冲突，也可能不冲突。在有些场景下，事实正确的外在幻觉可能会更好，但是事情往往并非总是如此。 \n\n\n\n\n\n\n\n重要\n\n\n\n幻觉，大模型的阿克琉斯之踵。\n\n\n更多关于幻觉的详细内容可以参见：[1]，[2]。\n\n\n\n\n[1] Ji, Z. 等 2023. Survey of Hallucination in Natural Language Generation. ACM Computing Surveys. 55, 12 (2023), 1–38. DOI:https://doi.org/10.1145/3571730.\n\n\n[2] Zhang, Y. 等 2023. Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models. arXiv preprint arXiv:2309.01219. (2023)."
  },
  {
    "objectID": "RAG_intro.html#rag-基本概念",
    "href": "RAG_intro.html#rag-基本概念",
    "title": "3  Retrieval Augmented Generation",
    "section": "3.1 RAG 基本概念",
    "text": "3.1 RAG 基本概念\n根据 A Survey on Retrieval-Augmented Text Generation [1] 所述：RAG 是深度学习和传统检索技术（Retrieval Technology）的有机结合，在生成式大模型时代，有着以下优势：\n\n知识库和模型分离，知识不以参数的形式存储在模型中，而是明文存储在数据库中，灵活性更高；\n文本生成转变为文本总结，生成结果的可信度更高，同时还降低了文本生成的难度；\n\n\n\n\n图 3.1: RATG 综述研究概览\n\n\n根据 图 3.1，RAG 范式有三个重要的组成部分：Retrieval Source，Retrieval Metric，Integration Method。\n\n3.1.1 RAG 的表示方法\n传统的文本生成方法可以用如下公式表示：\n\\[\\boldsymbol{y}=f(\\boldsymbol{x}) \\tag{3.1}\\]\n其中，\\(\\boldsymbol{x}\\) 代表输入的文本（字符串序列），\\(f\\) 表示模型，\\(\\boldsymbol{y}\\) 表示模型输出的文本。\nRAG 则可以用如下公式表示：\n\\[\\boldsymbol{y}=f(\\boldsymbol{x}, \\boldsymbol{z}), \\boldsymbol{z} = \\{(\\boldsymbol{x}^\\gamma, \\boldsymbol{y}^\\gamma)\\} \\tag{3.2}\\]\n其中，\\(\\boldsymbol{x}\\) 代表输入的文本（字符串序列），\\(\\boldsymbol{z}\\) 代表知识库，\\(f\\) 表示模型，\\(\\boldsymbol{x}^\\gamma\\) 表示作为输入文本 \\(\\boldsymbol{x}\\) 的检索 key，\\(\\boldsymbol{y}^\\gamma\\) 是与模型输出相关的知识。\n\n\n3.1.2 Retrieval Source 类型\n\nTraining Corpus：有标注的训练数据直接作为外部知识。\nExternal Data：支持提供训练数据之外的外部知识作为检索来源，比如于任务相关的领域数据，实现模型的快速适应。\nUnsupervised Data：前两种知识源都需要一定的人工标注来完善“检索依据-输出”的对齐工作，无监督知识源可以直接支持无标注/对齐的知识作为检索来源。\n\n\n\n3.1.3 Retrieval Metrics 类型\n\nSparse-vector Retrieval（浅层语义）：针对稀疏向量场景的度量方法，比如TF-IDF, BM25等。\nDense-vector Retrieval（深层语义）：针对稠密向量的度量方法，比如文本相似度。\nTask-specific Retrieval：在通用的度量场景下，度量得分高并不能代表召回知识准确，因此有学者提出基于特定任务优化的召回度量方法，提高度量的准确率。\n\n\n\n3.1.4 Integration Method 类型\n\nData Augmentation：直接拼接用户输入文本和知识文本，然后输入文本生成模型。\nAttention Mechanisms：引入额外的Encoder，对用户输入文本和知识文本进行注意力编码后输入文本生成模型。\nSkeleton Extraction：前两种方法都是通过文本向量化的隐式方法完成知识重点片段的抽取，Skeleton Extraction方法可以显式地完成类似工作。\n\n在 RAG 模式下，AI 应用发生了新的范式变化，从传统的 Pre-training + Fine-tune 的模式转换为了 Pre-training + Prompt 模式。这种模式的转变简化了对于不同任务而言模型训练的工作量，降低了 AI 的开发和使用门槛，同时也使得 Retriveval + Generation 成为可能。\n\n\n\n图 3.2: RAG 基本架构"
  },
  {
    "objectID": "RAG_intro.html#为什么要使用-rag",
    "href": "RAG_intro.html#为什么要使用-rag",
    "title": "3  Retrieval Augmented Generation",
    "section": "3.2 为什么要使用 RAG",
    "text": "3.2 为什么要使用 RAG\n仅依靠大模型已经可以完成很多任务，Fine-tune 也可以起到补充领域知识的作用，为什么 RAG 仍然如此重要呢？\n\n幻觉问题：尽管大模型的参数量很大，但和人类的所有知识相比，仍然有非常大的差距。所以，大模型在生成内容时，很有可能会捏造事实，导致如 章节 2 所述的“幻觉”。因此，对于 LLMs 而言，通过搜索召回相关领域知识来作为特定领域的知识补充是非常必要的。\n语料更新时效性问题：大模型的训练数据存在时间截止的问题。尽管可以通过 Fine-tune 来为大模型加入新的知识，但大模型的的训练成本和时间依然是需要面对的严峻难题：通常需要大量的计算资源，时间也难做到天级别更新。在 RAG 模式下，向量数据库和搜索引擎数据的更新都更加容易，这有助于业务数据的实时性。\n数据泄露问题：尽管，可以利用 Fine-tune 的方式增强 LLM 在特定领域的处理能力。但是，用于 Fine-tune 的这些领域知识很可能包含个人或者公司的机密信息，且这些数据很可能通过模型而不经意间泄露出去1。RAG 可以通过增加私有数据存储的方式使得用户的数据更加安全。"
  },
  {
    "objectID": "RAG_intro.html#更多内容",
    "href": "RAG_intro.html#更多内容",
    "title": "3  Retrieval Augmented Generation",
    "section": "3.3 更多内容",
    "text": "3.3 更多内容\n更详细、深入的内容可以参考如下几篇文章：[1]，[2]。"
  },
  {
    "objectID": "RAG_intro.html#参考文献",
    "href": "RAG_intro.html#参考文献",
    "title": "3  Retrieval Augmented Generation",
    "section": "3.4 参考文献",
    "text": "3.4 参考文献\n\n\n\n\n[1] Li, H. 等 2022. A Survey on Retrieval-Augmented Text Generation. arXiv preprint arXiv:2202.01110. (2022).\n\n\n[2] Mialon, G. 等 2023. Augmented Language Models: a Survey. (2023)."
  },
  {
    "objectID": "RAG_intro.html#footnotes",
    "href": "RAG_intro.html#footnotes",
    "title": "3  Retrieval Augmented Generation",
    "section": "",
    "text": "ChatGPT致三星半导体机密泄漏↩︎"
  },
  {
    "objectID": "agent_intro.html#阿克琉斯之踵",
    "href": "agent_intro.html#阿克琉斯之踵",
    "title": "4  Agent",
    "section": "4.1 阿克琉斯之踵",
    "text": "4.1 阿克琉斯之踵\n虽然 LLM 非常强大，但在某些方面，与“最简单”的计算机程序的能力相比，LLM 并没有表现的更好，例如在 计算 和 搜索 这些计算机比较擅长的场景下，LLM 的表现却却很吃力。\n\n列表 4.1: 文心大模型的计算能力测试\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for Ernie's calculate ability. \n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\n\nchat = ErnieBotChat()\nchain =  LLMChain(llm=chat, prompt=template)\nres =  chain.run(user_input=\"4.1*7.9=?\")\nprint(res)\n\n列表 4.1 的执行结果如下：\n['4.1乘以7.9等于31.79。']\n但实际上，\\(4.1 * 7.9 = 32.39\\)，很明显，文心给出了错误的结果。\n\n\n\n图 4.1: 一言的计算结果\n\n\n计算机程序（例如 python 的 mumexpr 库）可以轻而易举的处理这种简单的计算，甚至处理比这更复杂的计算也不在话下。但是，面对这些计算，LLM 有时候却显得力不从心。\n在 章节 3 中，我们提到，使用 RAG 可以解决训练数据的时效性问题、LLM 的幻觉问题、专有数据的安全性问题等问题，但是对于 列表 4.1 所示的问题，我们将如何解决？\n为了让 LLM 能更好的为我们赋能，我们必须解决这个问题，而接下来要介绍的 Agent 就是一种比较好的解决方案。\n利用 Agent，我们不但可以解决如上提到的 计算 的问题，我们还可以解决更多的问题。在我看来，Agent 可以解锁 LLM 的能力限制，让 LLM 具备无穷的力量，实现我们难以想象的事情。"
  },
  {
    "objectID": "agent_intro.html#什么是-agent",
    "href": "agent_intro.html#什么是-agent",
    "title": "4  Agent",
    "section": "4.2 什么是 Agent",
    "text": "4.2 什么是 Agent\n在日常生活中，我们解决问题也不是仅依靠我们自己的能力，我们也会使用计算器进行数学计算，我们也会 百度一下 以获取相关信息，君子性非异也善假于物也。同样，Agent 使得 LLM 可以像人一样做同样的事情。\n\n\n\n图 4.2: Agent 就是能够使用各种外部工具的 LLM\n\n\n从本质上讲，Agent 是一种特殊的 LLM，这种特殊的 LLM 的特殊性在于它可以使用各种外部工具来完成我们给定的操作。\n与我们使用外部工具完成任务一样：\n\n我们首先会对任务进行思考\n然后判断我们有哪些工具可用\n接下来再选择一种我们可用的工具来实施行动\n然后我们会观察行动结果以判断如何采取下一步的行动\n我们会重复 1-4 这个过程，直到我们认为我们完成了给定的任务\n\n\n\n\n图 4.3: Agent 流程示意图\n\n\n如 图 4.3 所示，虽然 Agent 本质上是 LLM，但是其包含的 Thought 和 Tools Set 将 Agent 和 LLM 区别开来，并且这种逐步思考的方式也使得 LLM 可以通过多次推理或多次使用工具来获取更好的结果。\n根据 B 站 UP 主发布的视频：作为一款优秀的 Agent，AutoGPT 可以实现自己查询文献、学习文献，并最终完成给定论文题目写作的整个过程，而整个过程中出了最开始需要给 AutoGPT 发布任务外，其他环节则全部由 AutoGPT 自动完成。"
  },
  {
    "objectID": "agent_intro.html#sec-agent_react",
    "href": "agent_intro.html#sec-agent_react",
    "title": "4  Agent",
    "section": "4.3 ReAct 模式",
    "text": "4.3 ReAct 模式\n此处的 ReAct 既不是软件设计模式中的 reactor 模式1，也不是 Meta 公司开发的前端开发框架 react2，而是 Yao 等人在 [1]，[3] 中提出的：把 Reasoning 和 Action 与语言模型结合起来的通用范式，以解决各种语言推理和决策任务。\nReAct 使 LLM 能够以交错的方式生成 reasoning traces 和 text actions，ReAct 可以从上下文中进行推理并提取有用的信息来进行后续的 reasoning 和 action，从而影响模型的内部状态。正如 [1] 说述，ReAct 将推理阶段和行动阶段进行有效的结合，进一步提升了 LLM 的性能。\n\n\n\n图 4.4: ReAct 模型\n\n\n实际上，和 图 4.3 所示的流程是一致的。\n\n\n\n\n\n\n注释\n\n\n\nReAct 也称为 Action Agent，在 ReAct 模式系下，代理的下一步动作由之前的输出来决定，其本质是对 Prompt 进行优化的结果，一般可以用于规模较小的任务。"
  },
  {
    "objectID": "agent_intro.html#planandexecute-模式",
    "href": "agent_intro.html#planandexecute-模式",
    "title": "4  Agent",
    "section": "4.4 PlanAndExecute 模式",
    "text": "4.4 PlanAndExecute 模式\n如前所述，Action Agent 适用于规模较小的任务。当任务规模较大，而任务的解决又高度依赖 Agent 来驱动并完成时，Action Agent 就开始变得捉襟见肘。\n我们即希望 Agent 能够处理更加复杂的任务，又希望 Agent 具备较高的稳定性和可靠性。这中既要又要的目标导致 Agent 的提示词变得越来越大，越来越复杂。\n\n为了解决更复杂的任务，我们需要更多的工具和推理步骤，这会导致 Agent 的提示词中包含了过多的历史推理信息\n同时，为了提升 Agent 的可靠性，需要不断的优化/增加 Tool 的描述，以便 LLM 可以选择正确的工具\n\n在这种背景下，PlanAndExecute 模式应运而生。PlanAndExecute 将 计划（plan） 与 执行（execute） 分离开来。\n在 PlanAndExecute 模式下，计划 由一个 LLM 来驱动生成，而 执行 则可以由另外的 Agent 来完成:\n\n首先，使用一个 LLM 创建一个用于解决当前请求的、具有明确步骤的计划。\n然后，使用传统的 Action Agent 来解决每个步骤。\n\n\n\n\n图 4.5: PlanAndExectue Agent 基本流程\n\n\n目前，BabyAGI 也采用了类似的模式3，更多关于 PlanAndExecute 模式的底层细节，可以参考 [2]。\n\n\n\n\n\n\n注释\n\n\n\n该模式下，代理将大型任务分解为较小的、可管理的子目标，从而可以高效处理复杂任务。\n这种方式可以通过 计划 让 LLM 更加“按部就班”，更加可靠。但是其代价在于，这种方法需要更多的 LLM 交互，也必然具有更高的延迟。4"
  },
  {
    "objectID": "agent_intro.html#参考文献",
    "href": "agent_intro.html#参考文献",
    "title": "4  Agent",
    "section": "4.5 参考文献",
    "text": "4.5 参考文献\n\n\n\n\n[1] ReAct: Synergizing Reasoning and Acting in Language Models: 2022. https://react-lm.github.io/.\n\n\n[2] Wang, L. 等 2023. Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.\n\n\n[3] Yao, S. 等 2022. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629. (2022)."
  },
  {
    "objectID": "agent_intro.html#footnotes",
    "href": "agent_intro.html#footnotes",
    "title": "4  Agent",
    "section": "",
    "text": "Reactor Pattern↩︎\nreact 官网↩︎\nBabyAGI↩︎\nPlan-and-Execute Agents↩︎"
  },
  {
    "objectID": "langchain_intro.html#langchain-的目标",
    "href": "langchain_intro.html#langchain-的目标",
    "title": "5  LangChain 简介",
    "section": "5.1 LangChain 的目标",
    "text": "5.1 LangChain 的目标\n不同的大语言模型都有各自的优势，我们可能会用 A 模型来进行自然语言理解，然后用 B 模型进行逻辑推理并获取结果……此时，如果使用大语言模型各自提供的 API 来和模型交互，那么就会存在非常多的重复工作。\n虽然大语言模型有很多，但是和大语言模型的交互流程又是非常类似（如 图 5.2 所示），如果每次和模型交互都需要重复如上的步骤，那听起来也是一件非常繁琐的事情。对于相同的提示词，我们不想每次都 ctr+c、ctr+v，这真是一件非常可怕的事情。\n\n\n\n\n\nflowchart LR\n  A(构造提示词) --&gt; B(LLMs)\n  B --&gt; C(模型生成结果)\n  C --&gt; D(结果处理)\n  D --&gt; E(最终结果)\n\n\n图 5.2: 和模型交互的流程\n\n\n\n\n和 FFmpeg 对视频的处理一样，FFmpeg 提供的 filtergraph 2机制大大增强了其音视频的处理能力，奠定其在视音频领域的地位。filtergraph 可以将不同的音视频处理能力以链条的形式组合起来，不但简化了音视频的处理流程，更让 FFmpeg 可以实现复杂的音视频处理。\n同理，和 LLMs 的单次交互并不会形成什么惊人的能量，而如果可以使用类似 filtergraph 的机制，将与 LLMs 的多次交互整合起来，那么其所释放的能量将是无穷的。\n而 LangChain 就是为了解决如上的问题而产生的。LangChain 可以提供给我们的最主要的价值如下3：\n\n组件化：LangChain 对与 LLMs 交互的流程进行了统一的抽象，同时也提供了不同 LLMs 的实现。这极大的提升了我们使用 LLMs 的效率。\n序列化：LangChain 提供的序列化的能力，可以将提示词、chain等以文件的形式而不是以代码的形式进行存储，这样可以极大的方便我们共享 提示词，并对 提示词 进行版本管理。4\n丰富的 chains 套件：LangChain 提供了丰富、用于完成特定目的、开箱即用的 chains 套件，例如用于总结文档的 StuffDocumentsChain 和 MapReduceDocumentsChain，这些套件将会降低我们使用 LLMs 的门槛。\n\n更具体的， LangChain 可以在如下的 6 大方向上给我们提供非常大的便利：\n\nLLMs & Prompt：LangChain 提供了目前市面上几乎所有 LLM 的通用接口，同时还提供了 提示词 的管理和优化能力，同时也提供了非常多的相关适用工具，以方便开发人员利用 LangChain 与 LLMs 进行交互。\nChains：LangChain 把 提示词、大语言模型、结果解析 封装成 Chain，并提供标准的接口，以便允许不同的 Chain 形成交互序列，为 AI 原生应用提供了端到端的 Chain。\nData Augemented Generation5：数据增强生成式 是一种解决预训练语料数据无法及时更新而带来的回答内容陈旧的方式。LangChain 提供了支持 数据增强生成式 的 Chain，在使用时，这些 Chain 会首先与外部数据源进行交互以获得对应数据，然后再利用获得的数据与 LLMs 进行交互。典型的应用场景如：基于特定数据源的问答机器人。\nAgent：对于一个任务，代理 主要涉及让 LLMs 来对任务进行拆分、执行该行动、并观察执行结果，代理 会重复执行这个过程，直到该任务完成为止。LangChain 为 代理 提供了标准接口，可供选择的代理，以及一些端到端的 代理 的示例。\nMemory：内存 指的是 chain 或 agent 调用之间的状态持久化。LangChain 为 内存 提供了标准接口，并提供了一系列的 内存 实现。\nEvaluation：LangChain 还提供了非常多的评估能力以允许我们可以更方便的对 LLMs 进行评估。\n\n\n\n\n\n\n\nLangChain 安装\n\n\n\nLangChain 的安装可以参见 附录 B。"
  },
  {
    "objectID": "langchain_intro.html#langchain-的基本概念",
    "href": "langchain_intro.html#langchain-的基本概念",
    "title": "5  LangChain 简介",
    "section": "5.2 LangChain 的基本概念",
    "text": "5.2 LangChain 的基本概念\n使用 LLMs 和使用电脑一样，需要一些基本的架构体系。LangChain 把整体架构体系分为两部分：输入/输出系统，大语言模型。其中，输入部分为 Prompt 相关组件，输出为 Output Parser 相关组件。具体参见 图 5.3。\n\n\n\n图 5.3: LangChain I/O\n\n\nLangChain 提供了与 LLMs 交互的通用构件：\n\nPrompts：提示词模版，提示词动态选择，提示词序列化。\nLLMs：与 LLM 交互的通用接口。\nOutput Parsers：对模型的输出信息进行解析，以输出符合特定格式的响应。\n\n\n\n\n图 5.4: LangChain I/O 示例\n\n\n\n5.2.1 Prompt Templates\n提示词模版为不同的提示词提供预定义格式。就好像目前超市售卖的洗净切好、配好相关配菜源材料的预制菜一样，提示词模版可以简化我们和 LLMs 交互的效率。\n模版会包含：指令，少量的样本示例，相关的上下文信息。如 章节 5.2.2 所述，LLMs 会分为 大语言模型 和 聊天模型 两种类型，因此，LangChain 提供了两种类型的提示词模版：prompt template、chat prompt template。\n\nprompt template：提供字符串格式的提示词。\nchat prompt template：提示聊天消息格式的提示词。\n\n\n列表 5.1: PromptTemplte 示例\nfrom langchain import PromptTemplate\n\nprompt_template = PromptTemplate.from_template(\n    \"请以轻松欢快的语气写一篇描写 {topic} 的文章，字数不超过 {count} 字。\"\n)\nres = prompt_template.format(topic=\"北京的秋天\", count=\"100\")\n\nprint(res)\n# 请以轻松欢快的语气写一篇描写 北京的秋天 的文章，字数不超过 100 字。\n\n\n列表 5.2: ChatPromptTemplte 示例\nfrom langchain.prompts import ChatPromptTemplate\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", \"你是一个能力非凡的人工智能机器人，你的名字是 {name}。\"),\n    (\"human\", \"你好！\"),\n    (\"ai\", \"你好~\"),\n    (\"human\", \"{user_input}\"),\n])\n\nmessages = template.format_messages(\n    name=\"小明\",\n    user_input=\"你是谁？\"\n)\n\nprint(messages)\n# [SystemMessage(content='你是一个能力非凡的人工智能机器人，你的名字是 小明。', \n#                additional_kwargs={}), \n# HumanMessage(content='你好！', additional_kwargs={}, example=False), \n# AIMessage(content='你好~', additional_kwargs={}, example=False), \n# HumanMessage(content='你是谁？', additional_kwargs={}, example=False)]\n\n\n\n5.2.2 LLMs\nLangChain 提供了两种模型的通用接口：\n\nLLMs：模型以字符串格式的提示词作为输入，并返回字符串格式的结果。\nChat models：其背后也是由某种 LLM 来支撑，但是以聊天消息列表格式的提示词作为输入，并返回聊天消息格式的结果。\n\n\n\n\n\n\n\nLLMs & Chat Models\n\n\n\nLLM 和 聊天模式 之间的区别虽然很微妙，但是却完全不同。\nLangChain 中的 LLM 指的是纯文本 I/O 的模型，其包装的 API 将字符串提示作为输入，并输出字符串。OpenAI 的 GPT-3 就是 LLM。\n聊天模型通常由 LLM 支持，但专门针对对话进行了调整，其 API 采用聊天消息列表作为输入，而不是单个字符串。通常，这些消息都标有角色（例如，“System”，“AI”，“Human”）。聊天模型会返回一条 AI 聊天消息作为输出。OpenAI 的 GPT-4，Anthropic 的 Claude，百度的 Ernie-Bot 都是聊天模型。\n\n\n在 LangChain 中，LLM 和 聊天模式两者都实现了 BaseLanguageModel 接口，因此一般情况下，这两种模型可以混用。例如，两种模型都实现了常见的方法 predict() 和 predict_messages()。predict() 接受字符串并返回字符串，predict_messages() 接受消息并返回消息。\n\n列表 5.3: LLM 模式\nclass OpenAI(BaseOpenAI):\n    # ...\n\nclass BaseOpenAI(BaseLLM):\n    # ...\n\nclass BaseLLM(BaseLanguageModel[str], ABC):\n    # ...\n\n\n列表 5.4: 聊天模型\nclass ErnieBotChat(BaseChatModel):\n    # ...\n\nclass BaseChatModel(BaseLanguageModel[BaseMessageChunk], ABC):\n    # ...\n\n接下来，我们将 Prompt 和 LLM 整合起来，实现和大语言模型交互。\n\n列表 5.5: LLM 模型示例\nfrom langchain import PromptTemplate\nfrom langchain.llms import OpenAI\n\nprompt_template = PromptTemplate.from_template(\n    \"请以轻松欢快的语气写一篇描写 {topic} 的文章，字数不超过 {count} 字。\"\n)\nllm = OpenAI()\n\nprompt = prompt_template.format(topic=\"北京的秋天\", count=\"100\")\nres = llm.predict(prompt)\nprint(res)\n\n# 秋天来到了北京，一片金黄色的枫叶，漫山遍野。\n# 湖面上的微风，吹起柔和的秋意，空气中弥漫着淡淡的枫香。\n# 这时，每一个角落都洋溢着秋日的温馨，令人心旷神怡。\n# 古老的长城上披着红叶，熙熙攘攘的人群中，也多了几分热闹与欢畅，这就是北京的秋天\n\n由于文心聊天模型对 message 角色和条数有限制6 7，因此我们需要对 提示词 做一些修改。\n\n列表 5.6: 聊天模型示例\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人，你的名字是 {name}。\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\nchat = ErnieBotChat()\n\nmessages = template.format_messages(\n    name=\"小明\",\n    user_input=\"你是谁？\"\n)\n\nres = chat.predict_messages(messages)\nprint(res)\n# content='我是你的新朋友小明，一个拥有先进人工智能技术的人工智能机器人。' \n# additional_kwargs={} example=False\n\n\n文心 4.0\n在 LangChain 中，要使用 文心 4.0 模型，可以在初始化 LLM 时设置 model_name 参数为 ERNIE-Bot-4。\nllm = ErnieBotChat(model_name=\"ERNIE-Bot-4\")\n\n\n\n5.2.3 Output Parsers\n大语言模型一般会输出文本内容作为响应，当然更高级的大语言模型（例如文心大模型）还可以输出图片、视频作为响应。但是，很多时候，我们希望可以获得更结构化的信息，而不仅仅是回复一串字符串文本。\n我们可以使用 提示词工程 来提示 LLMs 输出特定的格式，如 列表 5.7 所示：\n\n列表 5.7: 使用提示词工程来格式化输出内容\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人，你的名字是 {name}。\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\nchat = ErnieBotChat()\n\nmessages = template.format_messages(\n    name=\"小明\",\n    user_input=\"请给出 10 个表示快乐的成语，并输出为 JSON 格式\"\n)\n\nres = chat.predict_messages(messages)\nprint(res)\n\n# content='```json\\n[\\n    \"乐不可支\",\n#                    \\n    \"喜从天降\",\n#                    \\n    \"笑逐颜开\",\n#                    \\n    \"手舞足蹈\",\n#                     ......\n#                    \\n    \"弹冠相庆\"\\n]\\n```' \n# additional_kwargs={} example=False\n\n但是，使用 LangChain 提供的 Output Parsers 能力，会更加的方便。\n\n列表 5.8: 使用 Output Parser 解析 LLM 结果\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人，你的名字是 {name}。\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\nchat = ErnieBotChat()\n\nmessages = template.format_messages(\n    name=\"小明\",\n    user_input=\"请仅给5个表示快乐的成语并以 , 分隔，除了成语外不要输出任何其他内容\"\n)\n\nres = chat.predict_messages(messages)\nprint(res)\n# content='欢呼雀跃，手舞足蹈，笑逐颜开，心花怒放，喜笑颜开' additional_kwargs={} example=False\n\noutput_parser = CommaSeparatedListOutputParser()\nres =  output_parser.parse(res.content.replace('，', ', '))\nprint(res)\n# ['欢呼雀跃', '手舞足蹈', '笑逐颜开', '心花怒放', '喜笑颜开']\n\n\n\n\n\n\n\n警告\n\n\n\n由于文心大模型的指令遵循能力还有进一步提升的空间，因此这里的演示可能需要进行一些额外的操作，例如需要对模型返回的内容进行一些简单的字符串替换。\n2023 年 10 月 17 日，百度世界大会上发布了 文心 4.0，我们发现 文心 4.0 在 ICL、指令遵循、推理能力上都有比较大的提升。\n在 LangChain 中，要使用 文心 4.0 模型，可以在初始化 LLM 时设置 model_name 参数为 ERNIE-Bot-4。\nllm = ErnieBotChat(model_name=\"ERNIE-Bot-4\")\n\n\n\n\n5.2.4 LLMChain\n虽然一台独立的计算机也能实现很强大的功能，但是通过网络将更多的计算机链接起来，可能发挥出更大的性能。同样的，单独使用 LLMs 已经可以实现强大的功能，但是如果可以将更多次的交互有效的链接起来，则能发挥 LLMs 更大的能量。为了实现这个目标，LangChain 提供了 Chain 的概念，以实现对不同组件的一系列调用。\n在 LangChain 中，提示词、LLM、输出解析 这三者构成了 Chain，而不同的 Chain 则可以通过一定的方式链接起来，以实现强大的功能。具体如 图 5.5 所示。\n\n\n\n图 5.5: LangChain 中 Chain 的概念\n\n\n利用 Chain 的概念，我们可以对 列表 5.8 的代码进行重构，\n\n列表 5.9: 使用 chain 与文心大模型进行交互\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain.chains import LLMChain\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人，你的名字是 {name}。\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\nchat = ErnieBotChat()\n\nchain =  LLMChain(llm=chat, prompt=template, output_parser=CommaSeparatedListOutputParser())\n\nres =  chain.run(name=\"小明\", user_input=\"请仅给5个表示快乐的成语并以 , 分隔，除了成语外不要输出任何其他内容\")\n\nprint(res)\n# ['以下是五个表示快乐的成语：\\n\\n1. 喜出望外\\n2. 乐不可支\\n3. 心花怒放\\n4. 满心欢喜\\n5. 手舞足蹈']"
  },
  {
    "objectID": "langchain_intro.html#sec-langflow",
    "href": "langchain_intro.html#sec-langflow",
    "title": "5  LangChain 简介",
    "section": "5.3 Langflow",
    "text": "5.3 Langflow\nLangflow 是 LangChain 的非官方的 UI。使用 Langflow，我们可以更简便的以可视化的方式来体验 LangChain 并为基于 LangChain 的大语言应用提供原型设计能力。\n\n\n\n图 5.6: Langflow 示例\n\n\n想要深入了解 Langflow，可以阅读 Langflow 的官方文档。"
  },
  {
    "objectID": "langchain_intro.html#langchain-的学习资料",
    "href": "langchain_intro.html#langchain-的学习资料",
    "title": "5  LangChain 简介",
    "section": "5.4 LangChain 的学习资料",
    "text": "5.4 LangChain 的学习资料\n\nLangChain 官方文档：https://python.langchain.com/docs/get_started\nLangChain 的典型应用场景：https://python.langchain.com/docs/use_cases\nLangChain 目前集成的能力：https://python.langchain.com/docs/integrations\nLangChain AI Handbook：https://www.pinecone.io/learn/series/langchain/\nLangChain Dart：https://langchaindart.com/#/\n百度智能云千帆大模型平台：https://cloud.baidu.com/product/wenxinworkshop\nLangflow 官方文档：https://docs.langflow.org/"
  },
  {
    "objectID": "langchain_intro.html#参考文献",
    "href": "langchain_intro.html#参考文献",
    "title": "5  LangChain 简介",
    "section": "5.5 参考文献",
    "text": "5.5 参考文献"
  },
  {
    "objectID": "langchain_intro.html#footnotes",
    "href": "langchain_intro.html#footnotes",
    "title": "5  LangChain 简介",
    "section": "",
    "text": "LangChain 估值↩︎\nFFmpeg Filters Documentation↩︎\nLangChain Introdction↩︎\nPrompt Serialization↩︎\nA Complete Guide to Data Augmentation↩︎\nERNIE-Bot-turbo↩︎\n百度智能云千帆大模型平台↩︎"
  },
  {
    "objectID": "langchain_serialization.html#序列化",
    "href": "langchain_serialization.html#序列化",
    "title": "6  LangChain 序列化",
    "section": "6.1 序列化",
    "text": "6.1 序列化"
  },
  {
    "objectID": "langchain_serialization.html#langchain-hub",
    "href": "langchain_serialization.html#langchain-hub",
    "title": "6  LangChain 序列化",
    "section": "6.2 LangChain-Hub",
    "text": "6.2 LangChain-Hub"
  },
  {
    "objectID": "langchain_retrieval.html#document-loaders",
    "href": "langchain_retrieval.html#document-loaders",
    "title": "7  LangChain Retrieval",
    "section": "7.1 Document loaders",
    "text": "7.1 Document loaders\nLangChain 提供了100多种不同的文档加载器，并与该领域的其他主要供应商（如 AirByte、Unstructured）进行了集成，从而可以从任何地方（私有 s3 存储、网站）加载任何类型的文档（HTML、PDF、代码）。\n文档加载器提供了一个 load() 方法来从指定的加载源加载文档数据。文档加载器还提供了一个 lazy_load() 方法来实现现“延迟加载”，以避免一次将太多的数据加载到内存之中。\n\n列表 7.1: 加载远程网页\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n\nURL_ROOT = \"https://wangwei1237.github.io/\"\nloader = RecursiveUrlLoader(url=URL, max_depth=2)\ndocs = loader.load()\n\nprint(len(docs))\n\nURLS = []\nfor doc in docs:\n    url   =  doc.metadata[\"source\"]\n    title = doc.metadata[\"title\"]\n    print(url, \"-&gt;\", title)\n\n\n\n\n\n\n\n警告\n\n\n\nRecursiveUrlLoader() 对中文的抓取看起来不是非常友好，中文内容显示成了乱码。可以使用 列表 7.2 所示的方法来解决中文乱码的问题，不过这种方式的缺点是需要 load() 两次。更好的方式后续再思考。\n\n\n\n列表 7.2: 解决中文乱码的方法\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n\nURL_ROOT = \"https://wangwei1237.github.io/\"\nloader = RecursiveUrlLoader(url=URL_ROOT, max_depth=2)\ndocs = loader.load()\n\nprint(len(docs))\n\nURLS = []\nfor doc in docs:\n    url   =  doc.metadata[\"source\"]\n    URLS.append(url)\n\nloader = WebBaseLoader(URLS)\ndocs = loader.load()\n\nprint(len(docs))\n\nfor doc in docs:\n    url   =  doc.metadata[\"source\"]\n    title =  doc.metadata[\"title\"]\n    print(url, \"-&gt;\", title)"
  },
  {
    "objectID": "langchain_retrieval.html#document-transformers",
    "href": "langchain_retrieval.html#document-transformers",
    "title": "7  LangChain Retrieval",
    "section": "7.2 Document transformers",
    "text": "7.2 Document transformers\n检索的一个关键部分是只获取文档的相关部分而非获取全部文档。为了为最终的检索提供最好的文档，我们需要对文档进行很多的转换，这里的主要方法之一是将一个大文档进行拆分。LangChain 提供了多种不同的拆分算法，并且还针对特定文档类型（代码、标记等）的拆分提供对应的优化逻辑。\n文档加载后，我们通常会对文档进行一系列的转换，以更好地适应我们的应用程序。最简单的文档转换的场景就是文档拆分成，以便可以满足模型的上下文窗口（不同模型的每次交互的最大 token 数可能不同）。\n尽管文档拆分听起来很简单，但实际应用中却有很多潜在的复杂性。理想情况下，我们希望将语义相关的文本片段放在一起。“语义相关”的含义会取决于文本的类型，例如：\n\n对于代码文件而言，我们需要将一个函数置于一个完整的拆分块中；\n普通的文本而言，可能需要将一个段落置于一个完整的拆分块中；\n……\n\n我们利用 RecursiveCharacterTextSplitter 对 列表 7.2 的文档进行拆分。\n\n列表 7.3: 使用 RecursiveCharacterTextSplitter 拆分文档\n# ...\n# ...\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 1000,\n    chunk_overlap  = 20,\n    length_function = len,\n    add_start_index = True,\n)\n\nfor doc in docs:\n    url   =  doc.metadata[\"source\"]\n    title =  doc.metadata[\"title\"]\n    print(url, \"--&gt;\", title)\n    texts = text_splitter.create_documents([doc.page_content])\n    print(texts)\n\nLangChain 也可以对不同的编程语言进行拆分，例如 cpp，go，markdown，……，具体支持的语言可以参见 列表 7.4。\n\n列表 7.4: LangChain 支持拆分的语言类型\nfrom langchain.text_splitter import Language\n\n[e.value for e in Language]\n\n#['cpp',\n# 'go',\n# 'java',\n# 'js',\n# 'php',\n# 'proto',\n# 'python',\n# 'rst',\n# 'ruby',\n# 'rust',\n# 'scala',\n# 'swift',\n# 'markdown',\n# 'latex',\n# 'html',\n# 'sol']"
  },
  {
    "objectID": "langchain_retrieval.html#text-embedding-models",
    "href": "langchain_retrieval.html#text-embedding-models",
    "title": "7  LangChain Retrieval",
    "section": "7.3 Text embedding models",
    "text": "7.3 Text embedding models\n检索的另一个关键部分是为文档创建其向量（embedding）表示。Embedding 捕获文本的语义信息，使我们能够快速、高效地查找其他相似的文本片段。LangChain 集成了 25 种不同的 embedding 供应商和方法，我们可以根据我们的具体需求从中进行选择。LangChain 还提供了一个标准接口，允许我们可以便捷的在不同的 embedding 之间进行交换。\n在 LangChain 中，Embeddings 类是用于文本向量模型的接口。目前，有很多的向量模型供应商，例如：OpenAI，Cohere，Hugging Face，……Embeddings 类的目的就是为所有这些向量模型提供统一的、标准的接口。\nEmbeddings 类可以为一段文本创建对应的向量表示，从而允许我们可以在向量空间中去考虑文本。在向量空间中，我们还可以执行语义搜索，从而允许我们在向量空间中检索最相似的文本片段。\n因为不同的向量模型供应商对文档和查询采用了不同的向量方法，Embeddings 提供了两个方法：\n\nembed_documents()：用于文档向量化\nembed_query()：用于查询向量化\n\n\n列表 7.5: 使用文心大模型的 Embedding-V1 查询向量化\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint \n\nembeddings = QianfanEmbeddingsEndpoint()\nquery_result = embeddings.embed_query(\"你是谁？\")\nprint(query_result)\nprint(len(query_result))\n\n# [0.02949424833059311, -0.054236963391304016, -0.01735987327992916, \n#  0.06794580817222595, -0.00020318820315878838, 0.04264984279870987, \n#  -0.0661700889468193, ……\n# ……]\n# \n# 384\n\n\n列表 7.6: 使用文心大模型的 Embedding-V1 文档向量化\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint \n\nembeddings = QianfanEmbeddingsEndpoint()\ndocs_result = embeddings.embed_documents([\n    \"你谁谁？\",\n    \"我是百度的智能助手，小度\"\n])\nprint(len(docs_result), \":\" , len(docs_result[0]))\n\n# 2 : 384\n\n\n\n\n\n\n\n使用 QianfanEmbeddingsEndpoint 的注意事项\n\n\n\nLangChain 在 0.0.300 版本之后才支持 QianfanEmbeddingsEndpoint，并且 QianfanEmbeddingsEndpoint 还依赖 qianfan python 库的支持。\n因此，在使用 QianfanEmbeddingsEndpoint 之前，需要：\n\n升级 LangChain 的版本：pip install -U langchain。\n安装 qianfan 库：pip install qianfan。"
  },
  {
    "objectID": "langchain_retrieval.html#vector-stores",
    "href": "langchain_retrieval.html#vector-stores",
    "title": "7  LangChain Retrieval",
    "section": "7.4 Vector stores",
    "text": "7.4 Vector stores\n为文档创建 embedding 之后，需要对其进行存储并实现对这些 embedding 的有效搜索，此时我们需要向量数据库的支持。LangChain 集成了 50 多种不同的向量数据库，还提供了一个标准接口，允许我们轻松的在不同的向量存储之间进行切换。\n\n\n\n图 7.2: 向量数据库检索的基本流程\n\n\n这里，我们使用 Milvus 向量数据库来进行相关的演示。Milvus 安装和使用方式可以参见：附录 C。\n利用 Milvus 对 列表 7.6 进行优化：\n\n列表 7.7: 使用 Milvus 存储千帆 Embedding-V1 的结果\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Milvus\n\nurl = 'https://wangwei1237.github.io/2023/02/13/duzhiliao/'\nloader = WebBaseLoader([url])\ndocs  = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 200,\n    chunk_overlap  = 20,\n    length_function = len,\n    add_start_index = True,\n)\ntexts = text_splitter.create_documents([docs[0].page_content])\n\nvector_db = Milvus.from_documents(\n    texts,\n    QianfanEmbeddingsEndpoint(),\n    connection_args ={\"host\": \"127.0.0.1\", \"port\": \"8081\"},\n)\n\nquery = \"什么是度知了？\"\ndocs = vector_db.similarity_search(query)\nprint(docs)\n\n列表 7.7 的运行结果中，之所以会有两条重复的结果，是因为在执行文档向量化的时候，执行了两遍。在初始化 Milvus 实例时，如果只是查询操作，可以使用如下的方式：\n\n列表 7.8: Milvus 实例初始化\nvector_db = Milvus.from_documents(\n    [],\n    QianfanEmbeddingsEndpoint(),\n    connection_args ={\"host\": \"127.0.0.1\", \"port\": \"8081\"},\n)\n\nMilvus.from_documents 会创建一个名为 LangChainCollection 的 Collection。可以使用 milvus_cli 工具来查看该 Collection 的信息，也可以使用 Milvus 提供的 http 端口来查看相关信息：\nhttp://127.0.0.1:8081/v1/vector/collections/describe?collectionName=LangChainCollection\n\n\n\n\n\n\n修改 Collection 名称\n\n\n\n为了方便使用，可以使用 collection_name 参数以实现将不同的专有数据源存储在不同的 Collection。\nvector_db = Milvus.from_documents(\n    texts,\n    QianfanEmbeddingsEndpoint(),\n    connection_args={\"host\": \"127.0.0.1\", \"port\": \"8081\"},\n1    collection_name=\"test\",\n)\n\n1\n\n设置数据存储的 Collection，类似于在关系数据库中，将数据存储在不同的表中。\n\n\n\n\n\n\n\n\n\n\n警告\n\n\n\n使用千帆进行 Embedding 时，每次 Embedding 的 token 是有长度限制的，目前的最大限制是 384 个 token。因此，我们在使用 RecursiveCharacterTextSplitter 进行文档拆分的时候要特别注意拆分后文档的长度。\nqianfan.errors.APIError: api return error, \ncode: 336003, \nmsg: embeddings max tokens per batch size is 384\n\n\n在使用时，为了方便，我们可以把 embedding 和 query 拆分为两个部分：\n\n先将数据源进行向量化，然后存储到 Milvus 中\n检索的时候，直接从 Milvus 中检索相关信息\n\n对 列表 7.6 的代码进行优化：\n\n列表 7.9: 文档向量化后存入 Milvus\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for milvus embedding \n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Milvus\nimport time\n\nURL_ROOT = \"https://wangwei1237.github.io/2023/02/13/duzhiliao/\"\nloader = RecursiveUrlLoader(url=URL_ROOT, max_depth=2)\ndocs = loader.load()\n\nURLS = []\nfor doc in docs:\n    url   =  doc.metadata[\"source\"]\n    URLS.append(url)\n\nprint(\"URLS length: \", len(URLS))\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = 200,\n    chunk_overlap  = 20,\n    length_function = len,\n    add_start_index = True,\n)\n\nfor url in URLS:\n    print('-------------', url, '----------------')\n    loader = WebBaseLoader([url])\n    doc = loader.load()\n    texts = text_splitter.split_documents(doc)\n    vector_db = Milvus.from_documents(\n        texts,\n        QianfanEmbeddingsEndpoint(),\n        connection_args ={\"host\": \"127.0.0.1\", \"port\": \"8081\"},\n    )\n    print(\"    . insert \", len(texts), \" texts embeddings successful\")\n    time.sleep(5)\n\n检索相似内容的代码可以简化为：\n\n列表 7.10: 内容检索\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for milvus embedding query\n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint\nfrom langchain.vectorstores import Milvus\n\nvector_db = Milvus.from_documents(\n    [],\n    QianfanEmbeddingsEndpoint(),\n    connection_args ={\"host\": \"127.0.0.1\", \"port\": \"8081\"},\n)\n\nquery = \"什么是 RD曲线？\"\ndocs = vector_db.similarity_search(query)\nprint(docs)\n\n\n\n\n\n\n\n\n警告\n\n\n\n因为千帆向量化的 API 有 QPS 限制，因此，在使用千帆进行 embedding 时尽量控制一下 QPS。"
  },
  {
    "objectID": "langchain_retrieval.html#retrivers",
    "href": "langchain_retrieval.html#retrivers",
    "title": "7  LangChain Retrieval",
    "section": "7.5 Retrivers",
    "text": "7.5 Retrivers\n检索是 LangChain 花费精力最大的环节，LangChain 提供了许多不同的检索算法，LangChain 不但支持简单的语义检索，而且还增加了很多算法以提高语义检索的性能。\n一旦我们准备好了相关的数据，并且将这些数据存储到向量数据库（例如 Milvus），我们就可以配置一个 chain，并在 提示词 中包含这些相关数据，以便 LLM 在回答我们的问题时可以利用这些数据作为参考。\n对于参考外部数据源的 QA 而言，LangChain 提供了 4 种 chain：stuff，map_reduce，refine，map_rerank。stuff chain 把文档作为整体包含到 提示词 中，这只适用于小型文档。由于大多数 LLM 对 提示次 可以包含的 token 最大数量存在限制，因此建议使用其他三种类型的 chain。对于非 stuff chain，LangChain 将输入文档分割成更小的部分，并以不同的方式将它们提供给 LLM。这 4 种 chain 的具体信息和区别可以参见：docs/modules/chains/document。\n我们利用 QAWithSourcesChain 对 列表 7.10 进行优化，以实现一个完整的利用外部数据源的 Retrival Augment Generation（需要配合 列表 7.9）。\n\n列表 7.11: 基于 LangChain 和 Milvus 的 RAG\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for RAG \n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint\nfrom langchain.vectorstores import Milvus\n\nllm = ErnieBotChat()\nchain = load_qa_with_sources_chain(llm=llm, chain_type=\"refine\", return_intermediate_steps=True)\n\nquery = \"什么是度知了?\"\nvector_db = Milvus.from_documents(\n    [],\n    QianfanEmbeddingsEndpoint(),\n    connection_args ={\"host\": \"127.0.0.1\", \"port\": \"8081\"},\n)\n\ndocs = vector_db.similarity_search(query)\nprint(len(docs))\n\nres = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)\nprint(res)\n\n\n列表 7.11 的运行结果如下，结果包括 intermediate_steps 和 output_text：\n\nintermediate_steps 表示搜索过程中所指的文档\noutput_text 表示是问题的最终答案\n\n4\n\n{'intermediate_steps': \n    [\n        '根据提供的上下文信息，回答问题：\\n\\n「度知了」是一个在线问答平台，使用指南是由作者严丽编写的。该平台供了一个问答系统，用户可以在其中提出问题和获取答案。「度知了」的目的是帮助用户更好地理解和掌握知识，并提供了一个方便的途径来获取所需的信息。', \n        '根据提供的上下文信息，「度知了」是一个在线问答平台，使用指南是由作者严丽编写的。该平台提供了一个问答系统，用户可以在其中提出问题和获取答案。「度知了」的目的是帮助用户更好地理解和掌握知识，并提供了一个方便的途径来获取所需的信息。度知了基于ITU标准，依托自研的10+项专利技术，在不断实践的基础之上而形成的一款支持多端（PC，Android，iOS）评测的视频画质评测服务。\\n\\n因此，「度知了」是一个在线问答平台，提供视频画质评测服务。', \n        '根据提供的上下文信息，「度知了」是一个在线问答平台，提供视频画质评测服务。它基于ITU标准，依托自研的10+项专利技术，支持多端（PC，Android，iOS）评测。该平台旨在帮助用户更好地理解和掌握知识，并提供了一个方便的途径来获取所需的信息。「度知了」已上架各大商店应用市场，安卓端可通过华为应用商店、百度手机助手、小米应用商店、oppo应用商店、vivo应用商店直接搜索「度知了」进行安装。在APP端，用户可以通过快捷创建创建一个评测任务。', \n        \"Based on the new context, the existing answer is still accurate. The 'duzhiliao' in the original answer refers to the online platform 'Du Zhili', which provides video quality evaluation services. It is a multi-platform application (PC, Android, iOS) that uses 10+ self-developed patent technologies based on ITU standards to help users better understand and master knowledge, and provide a convenient way to obtain needed information. The platform has been uploaded to various store application markets, and users can install it through search for 'Du Zhili' on Huawei App Store, Baidu App Store, Xiaomi App Store, OPPO App Store, Vivo App Store. In the app, users can quickly create a review task.\"\n    ], \n    'output_text': \"Based on the new context, the existing answer is still accurate. The 'duzhiliao' in the original answer refers to the online platform 'Du Zhili', which provides video quality evaluation services. It is a multi-platform application (PC, Android, iOS) that uses 10+ self-developed patent technologies based on ITU standards to help users better understand and master knowledge, and provide a convenient way to obtain needed information. The platform has been uploaded to various store application markets, and users can install it through search for 'Du Zhili' on Huawei App Store, Baidu App Store, Xiaomi App Store, OPPO App Store, Vivo App Store. In the app, users can quickly create a review task.\"\n}\n为了显示 RAG 的优点，我们可以利用 列表 5.9 所示的代码向 LLM 问同样的问题：\nres =  chain.run(name=\"小明\", user_input=\"什么是度知了?\")\nprint(res)\n\n# ['度知了是一款智能问答产品，它能够理解并回答问题，提供信息和建议，主要应用在搜索、智能问答、智能语音交互等领域。\\n\\n度知了运用了文心大模型的能力，涵盖了海量数据，可以更好地理解和回答各种各样的问题。文心大模型是中国的一个大规模语言模型，它可以用于各种自然语言处理任务，包括文本分类、问答、文本摘要等。']"
  },
  {
    "objectID": "langchain_retrieval.html#retrievalqa",
    "href": "langchain_retrieval.html#retrievalqa",
    "title": "7  LangChain Retrieval",
    "section": "7.6 RetrievalQA",
    "text": "7.6 RetrievalQA\n使用 RetrievalQA 也可以实现 列表 7.11 同样的功能，并且代码整体会更简洁。\n\n列表 7.12: 基于 RetrievalQA 和 Milvus 的 RAG\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for RetrivalQA.\n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.embeddings import QianfanEmbeddingsEndpoint\nfrom langchain.vectorstores import Milvus\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores.base import VectorStoreRetriever\nfrom retrieval_prompt import PROMPT_SELECTOR\n\nretriever = VectorStoreRetriever(vectorstore=Milvus(embedding_function=QianfanEmbeddingsEndpoint(),\n1                                                    connection_args={\"host\": \"127.0.0.1\", \"port\": \"8081\"}))\n\nllm = ErnieBotChat()\n2prompt = PROMPT_SELECTOR.get_prompt(llm)\n3retrievalQA = RetrievalQA.from_llm(llm=llm, prompt=prompt, retriever=retriever)\n\nquery = \"什么是度知了?\"\n\n4res = retrievalQA.run(query)\nprint(res)\n\n\n1\n\n使用 Milvus 初始化向量检索器\n\n2\n\n因为文心对 MessageList 的限制，所以此处要重写 Prompt，否则执行时会报 Message 类型错误。具体提示词的修改可以参考：列表 7.13。\n\n3\n\n使用向量检索器初始化 RetrievalQA 实例\n\n4\n\n执行 RAG 检索并提炼最终结果\n\n\n\n列表 7.13: RetrievalQA 的提示词\n# flake8: noqa\n\n\"\"\"\n@discribe: prompt for test_retrievalQA.py.\n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    HumanMessagePromptTemplate,\n    AIMessagePromptTemplate,\n)\n\nprompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:\"\"\"\nPROMPT = PromptTemplate(\n    template=prompt_template, input_variables=[\"context\", \"question\"]\n)\n\nsystem_template = \"\"\"Use the following pieces of context to answer the users question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}\"\"\"\nmessages = [\n1    HumanMessagePromptTemplate.from_template(system_template),\n2    AIMessagePromptTemplate.from_template(\"OK!\"),\n    HumanMessagePromptTemplate.from_template(\"{question}\"),\n]\nCHAT_PROMPT = ChatPromptTemplate.from_messages(messages)\n\n\nPROMPT_SELECTOR = ConditionalPromptSelector(\n    default_prompt=PROMPT, conditionals=[(is_chat_model, CHAT_PROMPT)]\n)\n\n\n1\n\n修改 SystemMessagePromptTemplate 为 HumanMessagePromptTemplate。\n\n2\n\n增加一条 AIMessagePromptTemplate 消息。\n\n\n列表 7.12 的运行结果如下所示：\n度知了是一款视频画质评测服务，基于ITU标准，依托自研的10+项专利技术，支持多端（PC、Android、iOS）评测，提供画质评测工具。"
  },
  {
    "objectID": "langchain_function_call.html#大模型的时效性",
    "href": "langchain_function_call.html#大模型的时效性",
    "title": "8  LangChain 函数调用",
    "section": "8.1 大模型的时效性",
    "text": "8.1 大模型的时效性\n当我们问大模型“明天天气怎么样”时，因为大模型训练语料的时效性问题，如果不依赖外部信息，大模型是很难回答这种问题的，如 图 8.1 所示。\n\n\n\n\n\n\n\n(a) ChatGPT\n\n\n\n\n\n\n\n(b) 文心一言\n\n\n\n\n图 8.1: 明天天气怎么样？\n\n\n而 OpenAI 大语言模型提供的 函数调用 能力，恰恰非常完美的解决了类似的问题，从而使得大语言模型可以通过 函数调用 与外部系统通信，并获取更实时的信息，以解决类似的问题。"
  },
  {
    "objectID": "langchain_function_call.html#函数调用流程",
    "href": "langchain_function_call.html#函数调用流程",
    "title": "8  LangChain 函数调用",
    "section": "8.2 函数调用流程",
    "text": "8.2 函数调用流程\nOpenAI 开发的大语言模型（例如GPT-3.5-turbo-0613，GPT-4-0613）提供了一种名为 Function Calling(函数调用) 的创新功能。函数调用 使得开发人员能够在模型中对函数进行描述，然后模型可以利用这些描述来巧妙地为函数生成调用参数。\n在 OpenAI 中，函数调用的步骤可以参考：图 8.2\n\n\n\n图 8.2: OpenAI 的函数调用流程\n\n\n\n\n\n\n\n\n注意\n\n\n\n需要特别注意的是，大语言模型本身并不会调用我们预定的 函数，大语言模型仅仅是生成我们所要调用的函数的调用参数而言，具体调用函数的动作，需要我们在自己的应用代码中来实现。2\n\n\n\n\n\n\n\n\n思考\n\n\n\n为什么模型不能直接调用函数？\n\n\n利用 函数调用，LLMs 可以很方便的将自然语言指令转变为相关的函数调用，例如：可以把“给张三发一封邮件询问下他下周五下午是否需要一杯咖啡” 这样的提示转换为 send_email(to: string, body: string) 函数调用。"
  },
  {
    "objectID": "langchain_function_call.html#示例",
    "href": "langchain_function_call.html#示例",
    "title": "8  LangChain 函数调用",
    "section": "8.3 示例",
    "text": "8.3 示例\n\n8.3.1 OpenAI API\n\n列表 8.1: 使用 OpenAI API 进行函数调用示例\nimport openai\nimport json\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit=\"celsius\"):\n    \"\"\"Get the current weather in a given location\"\"\"\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"27\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{\"role\": \"user\", \"content\": \"北京明天天气怎么样?\"}]\n    functions = [\n        {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-0613\",\n        messages=messages,\n        functions=functions,\n        function_call=\"auto\",  # auto is default, but we'll be explicit\n    )\n\n    print(\"---------step 1. the 1st LLMs response-----------\")\n    print(response)\n\n    response_message = response[\"choices\"][0][\"message\"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get(\"function_call\"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            \"get_current_weather\": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message[\"function_call\"][\"name\"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n        function_response = fuction_to_call(\n            location=function_args.get(\"location\"),\n            #unit=function_args.get(\"unit\"),\n        )\n\n        print(\"---------step 2. function response-----------\")\n        print(function_response)\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant's reply\n        messages.append(\n            {\n                \"role\": \"function\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )  # extend conversation with function response\n\n        print(\"---------step 3. final messages-----------\")\n        print(messages)\n\n        second_response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo-0613\",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\nres = run_conversation()\nprint(\"---------step 4. final LLMs response-----------\")\nprint(res)\n\n列表 8.1 的运行结果如 列表 8.2：\n\n列表 8.2: 运行结果\n---------step 1. the 1st LLMs response-----------\n{\n  \"id\": \"chatcmpl-7xnsEW2rSsec7Qd1FC60cKIT7TtuR\",\n  \"object\": \"chat.completion\",\n  \"created\": 1694487422,\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": null,\n        \"function_call\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": \"{\\n  \\\"location\\\": \\\"北京\\\"\\n}\"\n        }\n      },\n      \"finish_reason\": \"function_call\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 85,\n    \"completion_tokens\": 16,\n    \"total_tokens\": 101\n  }\n}\n---------step 2. function response-----------\n{\"location\": \"北京\", \"temperature\": \"27\", \"unit\": null, \"forecast\": [\"sunny\", \"windy\"]}\n---------step 3. final messages-----------\n[{'role': 'user', 'content': '北京明天天气怎么样?'}, &lt;OpenAIObject at 0x1082907c0&gt; JSON: {\n  \"role\": \"assistant\",\n  \"content\": null,\n  \"function_call\": {\n    \"name\": \"get_current_weather\",\n    \"arguments\": \"{\\n  \\\"location\\\": \\\"北京\\\"\\n}\"\n  }\n}, {'role': 'function', 'name': 'get_current_weather', 'content': '{\"location\": \"\\\\u5317\\\\u4eac\", \"temperature\": \"27\", \"unit\": null, \"forecast\": [\"sunny\", \"windy\"]}'}]\n---------step 4. final LLMs response-----------\n{\n  \"id\": \"chatcmpl-7xnsFw2dssMs3R0aGVMmjB0cjLugZ\",\n  \"object\": \"chat.completion\",\n  \"created\": 1694487423,\n  \"model\": \"gpt-3.5-turbo-0613\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"北京明天的天气预报是晴天，有很大的风。气温为27°C。\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 77,\n    \"completion_tokens\": 30,\n    \"total_tokens\": 107\n  }\n}\n\n\n\n8.3.2 LangChain 中调用 OpenAI Functions\n可以参考 LangChain 官方文档以在 LangChain 中使用 OpenAI 函数调用 的能力。3\n\n列表 8.3: 使用 LangChain 实现函数调用\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains.openai_functions import (\n    create_openai_fn_chain,\n)\nfrom langchain.chains import LLMChain\nimport json\n\ndef get_current_weather(location: str, unit: str=\"celsius\") -&gt; str:\n    \"\"\"Get the current weather in a given location\n\n    Args:\n        location (str): location of the weather.\n        unit (str): unit of the tempuature.\n    \n    Returns:\n        str: weather in the given location.\n    \"\"\"\n\n    weather_info = {\n        \"location\": location,\n        \"temperature\": \"27\",\n        \"unit\": unit,\n        \"forecast\": [\"sunny\", \"windy\"],\n    }\n    return json.dumps(weather_info)\n\nllm = ChatOpenAI(model=\"gpt-3.5-turbo-0613\")\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"human\", \"{query}\"),\n    ]\n)\n\nchain = create_openai_fn_chain([get_current_weather], llm, prompt, verbose=True)\nres = chain.run(\"What's the weather like in Beijing tomorrow?\")\nprint(\"-------------The 1-st langchain result-------------\")\nprint(res)\n\nres_func = get_current_weather(res['location'])\n\nchain = LLMChain(llm=llm, prompt=prompt, verbose=True)\nres = chain.run(\"extract the tomorrow weather infomation from ：%s， and answer the question: %s\" % (res_func, \"What's the weather like in Beijing tomorrow?\"))\nprint(res)\n\n列表 8.3 的运行结果如下所示：\n\n列表 8.4: 运行结果\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nHuman: What's the weather like in Beijing tomorrow?\n\n&gt; Finished chain.\n-------------The 1-st langchain result-------------\n{'location': 'Beijing', 'unit': 'metric'}\n\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nHuman: extract the tomorrow weather infomation from ：{\"location\": \"Beijing\", \"temperature\": \"27\", \"unit\": \"celsius\", \"forecast\": [\"sunny\", \"windy\"]}， and answer the question: What's the weather like in Beijing tomorrow?\n\n&gt; Finished chain.\nThe weather in Beijing tomorrow is sunny and windy.\n\n\n\n\n\n\n\n注释\n\n\n\n在 create_openai_fn_chain 中，其第一个参数是一个函数列表，如果该列表只有 1 个函数时，则 create_openai_fn_chain 仅会返回大语言模型构造的调用该函数对应的参数。例如如上的例子，create_openai_fn_chain 仅返回了 {'location': 'Beijing', 'unit': 'metric'}。 而如果函数列表存在多个函数时，则会返回大语言模型分析之后所需要调用的函数名以及对应的参数，例如： {'name': 'get_current_weather', 'arguments': {'location': 'Beijing'}}。\n\n\n\n列表 8.5: create_openai_fn_chain() 传递多个函数调用示例\n# ...\ndef get_current_news(location: str) -&gt; str:\n    \"\"\"Get the current news based on the location.'\n\n    Args:\n        location (str): The location to query.\n    \n    Returs:\n        str: Current news based on the location.\n    \"\"\"\n\n    news_info = {\n        \"location\": location,\n        \"news\": [\n            \"I have a Book.\",\n            \"It's a nice day, today.\"\n        ]\n    }\n\n    return json.dumps(news_info)\n# ...\n\nchain = create_openai_fn_chain([get_current_weather, get_current_news], llm, prompt, verbose=True)\nres = chain.run(\"What's the weather like in Beijing tomorrow?\")\nprint(\"-------------The 1-st langchain result-------------\")\nprint(res)\n\n列表 8.5 的运行结果如 列表 8.6 所示：\n\n列表 8.6: 运行结果\n&gt; Entering new LLMChain chain...\nPrompt after formatting:\nHuman: What's the weather like in Beijing tomorrow?\n\n&gt; Finished chain.\n-------------The 1-st langchain result-------------\n{'name': 'get_current_weather', 'arguments': {'location': 'Beijing'}}"
  },
  {
    "objectID": "langchain_function_call.html#参考文献",
    "href": "langchain_function_call.html#参考文献",
    "title": "8  LangChain 函数调用",
    "section": "8.4 参考文献",
    "text": "8.4 参考文献"
  },
  {
    "objectID": "langchain_function_call.html#footnotes",
    "href": "langchain_function_call.html#footnotes",
    "title": "8  LangChain 函数调用",
    "section": "",
    "text": "Function calling and other API updates↩︎\nGuides: Function calling↩︎\nUsing OpenAI functions↩︎"
  },
  {
    "objectID": "langchain_agent_react.html#三大基本组件",
    "href": "langchain_agent_react.html#三大基本组件",
    "title": "9  LangChain ReAct Agent",
    "section": "9.1 三大基本组件",
    "text": "9.1 三大基本组件\n在 LangChain 中，要使用 Agent，我们需要三大基本组件：\n\n一个基本的 LLM\n一系列 Tool，LLM 可以与这些工具进行交互\n一个 Agent，用于控制 LLM 和工具之间的交互\n\n\n列表 9.1: 用于初始化 Agent 的函数\n# path: langchain/agent/initialize.py\n\ndef initialize_agent(\n    tools: Sequence[BaseTool],\n    llm: BaseLanguageModel,\n    agent: Optional[AgentType] = None,\n    callback_manager: Optional[BaseCallbackManager] = None,\n    agent_path: Optional[str] = None,\n    agent_kwargs: Optional[dict] = None,\n    *,\n    tags: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -&gt; AgentExecutor\n\n\n9.1.1 初始化 LLM\n首先，我们使用 ErnieBot 来初始化一个基本 LLM。\n\n列表 9.2: 初始化基本 LLM\nfrom langchain.chat_models import ErnieBotChat\n\nllm = ErnieBotChat()\n\n\n\n9.1.2 初始化 Tool\n然后，我们来初始化工具。在初始化工具时，我们要么创建自定义的工具，要么加载 LangChain 已经构建好的工具。不管是以哪种方式初始化工具，在 LangChain 中，工具都是一个包含 name 和 description 属性的具备某种特定能力的 Chain。\n我们可以使用 LangChain 提供的 LLMMathChain 来构造一个用于计算数学表达式的工具。\n\n列表 9.3: 初始化数学计算工具\nfrom langchain.chains import LLMMathChain\nfrom langchain.agents import Tool\n\nllm_math = LLMMathChain(llm=llm)\n\n# initialize the math tool\nmath_tool = Tool(\n    name='Calculator',\n    func=llm_math.run,\n    description='Useful for when you need to answer questions about math.'\n)\n\ntools = [math_tool]\n\n\n\n\n\n\n\n提示\n\n\n\n在初始化工具时，要特别注意对 description 属性的赋值。因为 Agent 主要根据该属性值来判断接下来将要采用哪个工具来执行后续的操作。优秀的 description 有利于最终任务的完美解决。\n\n\n当然，LangChain 为我们提供了构建好的 llm_math 工具，我们可以使用如下的方式直接加载：\n\n列表 9.4: 使用 load_tools() 初始化数学计算工具\nfrom langchain.agents import load_tools\n\ntools = load_tools(\n    ['llm-math'],\n    llm=llm\n)\n\n如果查看一下 langchain/agents/load_tools.py 中对 load_tools() 的定义，我们会发现，LangChain 提供的预定义的工具和我们在 列表 9.3 中自己定义的工具是基本一致的：\n\n列表 9.5: _get_llm_math() 创建数学计算工具\ndef _get_llm_math(llm: BaseLanguageModel) -&gt; BaseTool:\n    return Tool(\n        name=\"Calculator\",\n        description=\"Useful for when you need to answer questions about math.\",\n        func=LLMMathChain.from_llm(llm=llm).run,\n        coroutine=LLMMathChain.from_llm(llm=llm).arun,\n    )\n\n\n\n\n\n\n\n提示\n\n\n\n可以通过调用 get_all_tool_names() 来获取 LangChain 支持的所有的预定义的工具，该函数的实现位于 langchain/agents/load_tools.py。\ndef get_all_tool_names() -&gt; List[str]:\n    \"\"\"Get a list of all possible tool names.\"\"\"\n    return (\n        list(_BASE_TOOLS)\n        + list(_EXTRA_OPTIONAL_TOOLS)\n        + list(_EXTRA_LLM_TOOLS)\n        + list(_LLM_TOOLS)\n    )\n\n\n\n\n9.1.3 初始化 Agent\n在 LangChain 中，可以使用 列表 9.1 所示的 initialize_agent 来初始化 Agent：\n\n列表 9.6: 初始化 Agent\nfrom langchain.agents import initialize_agent\n\nzero_shot_agent = initialize_agent(\n    agent=\"zero-shot-react-description\",\n    tools=tools,\n    llm=llm,\n    verbose=True,\n    max_iterations=3\n)\n\n列表 9.6 中使用 zero-shot-react-description 初始化了一个 zero-shot Agent。zero-shot 意味着该 Agent 仅会根据当前的行为来其作用，它是一个无状态的、无记忆能力的 Agent，无法根据历史的行为起作用。该 Agent 会根据我们在 章节 4.3 中提到的 ReAct 模式并根据当前的行为来判断接下来要调用哪个工具来完成任务。如前所述，Agent 主要根据 Tool.description 决策调用哪个工具，因此，务必保证改描述的准确性。\n\nAgent 类型\n想要了解 LangChain 支持的 Agent 类型，可以参考 langchain/agent/agent_types.py 文件：\nclass AgentType(str, Enum):\n    \"\"\"Enumerator with the Agent types.\"\"\"\n\n    ZERO_SHOT_REACT_DESCRIPTION = \"zero-shot-react-description\"\n    REACT_DOCSTORE = \"react-docstore\"\n    SELF_ASK_WITH_SEARCH = \"self-ask-with-search\"\n    CONVERSATIONAL_REACT_DESCRIPTION = \"conversational-react-description\"\n    CHAT_ZERO_SHOT_REACT_DESCRIPTION = \"chat-zero-shot-react-description\"\n    CHAT_CONVERSATIONAL_REACT_DESCRIPTION = \"chat-conversational-react-description\"\n    STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION = (\n        \"structured-chat-zero-shot-react-description\"\n    )\n    OPENAI_FUNCTIONS = \"openai-functions\"\n    OPENAI_MULTI_FUNCTIONS = \"openai-multi-functions\"\n而不同的 Agent 类型和具体的实现之间的映射关系位于 langchain/agent/types.py 的 AGENT_TO_CLASS 字典中。\n\n列表 9.7: Agent 类型和具体实现的映射关系\nAGENT_TO_CLASS: Dict[AgentType, AGENT_TYPE] = {\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\n    AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n    AgentType.OPENAI_FUNCTIONS: OpenAIFunctionsAgent,\n    AgentType.OPENAI_MULTI_FUNCTIONS: OpenAIMultiFunctionsAgent,\n}\n\n由此可知，zero-shot-react-description Agent 的定义位于 langchain/agents/mrkl/base.py 中的 ZeroShotAgent 类。\n\n\n\n\n\n\n提示\n\n\n\nMRKL 是 Modular Reasoning, Knowledge and Language 的简称，该系统的详细信息参见 [1]。"
  },
  {
    "objectID": "langchain_agent_react.html#zero-shot-agent",
    "href": "langchain_agent_react.html#zero-shot-agent",
    "title": "9  LangChain ReAct Agent",
    "section": "9.2 Zero Shot Agent",
    "text": "9.2 Zero Shot Agent\n我们将如上的三大组件整合起来，得到了一个简单的 zero shot Agent 的例子：\n\n列表 9.8: zero-shot Agent\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for zero shot agent.\n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chains import LLMMathChain\nfrom langchain.agents import Tool\nfrom langchain.agents import initialize_agent\n\nllm = ErnieBotChat()\nllm_math = LLMMathChain(llm=llm)\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人。\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\nllm_chain = LLMChain(llm=llm, prompt=template)\n\n# initialize the math tool\nmath_tool = Tool(\n    name='Calculator',\n    func=llm_math.run,\n    description='Useful for when you need to answer questions about math.'\n)\n\n# initialize the general LLM tool\nllm_tool = Tool(\n    name='Language Model',\n    func=llm_chain.run,\n    description='use this tool for general purpose queries.'\n)\n\n# when giving tools to LLM, we must pass as list of tools\ntools = [math_tool, llm_tool]\n\nzero_shot_agent = initialize_agent(\n    agent=\"zero-shot-react-description\",\n    tools=tools,\n    llm=llm,\n    verbose=True,\n    max_iterations=3\n)\n\nres = zero_shot_agent(\"what's 4.1*7.9=?\")\nprint(res)\n\n列表 9.8 的运行结果如下：\n&gt; Entering new AgentExecutor chain...\nAction: Calculator\nAction Input: 4.1*7.9\nObservation: 32.39\nThought: I'm happy with the result\nFinal Answer: 32.39\n我们可以继续问其他的问题：\nres = zero_shot_agent(\"what's the capital of China?\")\nprint(res)\n\n#Action: Calculator\n#Action Input: country code + search term (capital)\n#Observation: capital of China is Beijing\n#Thought: hmm... looks good. Let's think of another question\n#Action: Language Model\n#Action Input: weather in Beijing\n#Observation: the weather in Beijing is usually good\n#Thought: alright, seems like that question is also answered well\n#Final Answer: The capital of China is Beijing and the weather is usually good there.\n\n\n\n\n\n\n警告\n\n\n\n当然，在解决实际问题中，Agent 的 ReAct 过程可能会有差异，这些差异可能是因为 LLM 的能力导致的，例如指令遵循的能力，上下文学习的能力等。\n在我使用文心的过程中，经常会报如下的异常：\n\n列表 9.9: Agent 执行异常的问题\nraise OutputParserException(\nlangchain.schema.output_parser.OutputParserException: Parsing LLM output produced both a final answer and a parse-able action:: Thought: what's 3*4? - You should always think about what to do\nAction: use calculator\nAction Input: 3*4\nObservation: 12\n...\nThought: Good, moving on\nFinal Answer: 12\n\n如异常信息所示，异常的原因是因为 Agent 在解析文心大模型的返回结果时，当大模型给出了 Action 之后，同时又给出了 Final Answer。哎呀，真是头疼，LLM 即给了接下来要调用 calculator 来完成任务，但是呢，Agent 还没有调用的时候，LLM 直接给了 Final Answer，那 Agent 的作用不就完全丧失了吗？这完全不按套路出牌呀！\n值得兴奋的是，2023 年 10 月 17 日，百度世界大会上发布了 文心 4.0，我们发现 文心 4.0 在 ICL、指令遵循、推理能力上都有比较大的提升。而 文心 4.0 也比较好的解决了如上的推理问题。\n\n\n\n9.2.1 深入 Zero Shot Agent\n我们之前说过，Agent 本质上也是一个 chain，那么我们来看下 Zero Shot Agent 的提示词究竟是怎么实现 推理 -&gt; 行动 -&gt; 行动输入 -&gt; 观察结果 这个循环的。\n可以使用如下代码来显示 Agent 的提示词：\nprint(zero_shot_agent.agent.llm_chain.prompt.template)\n\n列表 9.10: Zero Shot Agent 的提示词\nAnswer the following questions as best you can. You have access to the following tools:\n\nCalculator: Useful for when you need to answer questions about math.\nLanguage Model: use this tool for general purpose queries and logic\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [Calculator, Language Model]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: {input}\nThought:{agent_scratchpad}\n\n根据上面的提示词，我们也能发现，文心大模型确实没有很好的进行指令遵循。所幸的是，2023 年 10 月 17 日，百度世界大会上发布了 文心 4.0，我们发现 文心 4.0 在 ICL、指令遵循、推理能力上都有比较大的提升。如上的提示词的生成逻辑可以参见：langchain/agents/mrkl/base.py 中的 create_prompt()。\n\n\n\n\n\n\n提示\n\n\n\n在 列表 9.10 中，提示词的最后一行是 Thought:{agent_scratchpad}。 agent_scratchpad 保存了代理已经执行的所有想法或行动，下一次的 思考 -&gt; 行动 -&gt; 观察 循环可以通过 agent_scratchpad 访问到历史的所有想法和行动，从而实现代理行动的连续性。"
  },
  {
    "objectID": "langchain_agent_react.html#conversational-agent",
    "href": "langchain_agent_react.html#conversational-agent",
    "title": "9  LangChain ReAct Agent",
    "section": "9.3 Conversational Agent",
    "text": "9.3 Conversational Agent\nZero Shot Agent 虽然可以解决很多场景下的任务，但是它没有会话记忆的能力。对于聊天机器人之类的应用而言，缺乏记忆能力可能会成为问题。例如，如下的连续对话：\n\n1768年，中国有什么重大事件发生？\n同年，其他国家有什么重大事件发生？\n\n幸运的是，LangChain 为我们提供了支持记忆能力的 Agent，可以使用 conversational-react-description 来初始化具备记忆能力的 Agent。除了拥有记忆之外，Conversational Agent 和 Zero Shot Agent 是一致的。\n\n列表 9.11: Conversation Agent\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for conversation agent.\n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chains import LLMMathChain\nfrom langchain.agents import Tool\n1from langchain.memory import ConversationBufferMemory\nfrom langchain.agents import initialize_agent\n\n2memory = ConversationBufferMemory(memory_key=\"chat_history\")\n\nllm = ErnieBotChat()\nllm_math = LLMMathChain(llm=llm)\n\ntemplate = ChatPromptTemplate.from_messages([\n    (\"user\", \"你是一个能力非凡的人工智能机器人。\"),\n    (\"assistant\", \"你好~\"),\n    (\"user\", \"{user_input}\"),\n])\nllm_chain = LLMChain(llm=llm, prompt=template)\n\n# initialize the math tool\nmath_tool = Tool(\n    name='Calculator',\n    func=llm_math.run,\n    description='Useful for when you need to answer questions about math.'\n)\n\n# initialize the general LLM tool\nllm_tool = Tool(\n    name='Language Model',\n    func=llm_chain.run,\n    description='Use this tool for general purpose queries.'\n)\n\n# when giving tools to LLM, we must pass as list of tools\ntools = [math_tool, llm_tool]\n\nconversation_agent = initialize_agent(\n3    agent=\"conversational-react-description\",\n    tools=tools,\n    llm=llm,\n    verbose=True,\n    max_iterations=3,\n4    memory=memory\n)\n\nres = conversation_agent(\"1768年，中国有什么重大事件发生？\")\nprint(res)\n\nres = conversation_agent(\"同年，其他国家有什么重大事件发生？\")\nprint(res)\n\n\n1\n\n引入 ConversationBufferMemory 类\n\n2\n\n使用 ConversationBufferMemory 来初始化用于存储会话历史的 memory\n\n3\n\n在 initialize_agent 时，指定 Agent 类型为 conversational-react-description\n\n4\n\n为 Agent 配置 memory\n\n\n根据 列表 9.7，Conversation Agent 的具体实现为 langchain/agent/conversation/base.py 中的 ConversationalAgent 类。\n同样，我们使用如下代码来看一下 Conversation Agent 的提示词：\nprint(conversation_agent.agent.llm_chain.prompt.template)\n\n列表 9.12: Conversation Agent 的提示词\n1Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\nTOOLS:\n------\n\nAssistant has access to the following tools:\n\n&gt; Calculator: Useful for when you need to answer questions about math.\n&gt; Language Model: Use this tool for general purpose queries.\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [Calculator, Language Model]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nBegin!\n\nPrevious conversation history:\n2{chat_history}\n\nNew input: {input}\n{agent_scratchpad}\n\n\n1\n\n作为一个通用的框架，在提示词中这样写其实不是特别合理。\n\n2\n\n存储历史对话消息的地方，当我们问 &lt;同年，其他国家有什么重大事件发生？&gt;时，Agent 可以从这里获取知识，以推理出 &lt;1768年，中国之外有什么重大事件发生？&gt;。"
  },
  {
    "objectID": "langchain_agent_react.html#agent-提示词工程",
    "href": "langchain_agent_react.html#agent-提示词工程",
    "title": "9  LangChain ReAct Agent",
    "section": "9.4 Agent 提示词工程",
    "text": "9.4 Agent 提示词工程\n现在，如果让 Agent 解决数学问题：\nres = conversation_agent(\"what is 3*4?\")\n我们会发现，Agent 依然会出现 列表 9.9 所示的问题。如前所述，这里和我们所使用的 LLM 的能力有关系，另外的原因还在于 LLM 有时候有可能过分自信，所以当需要使用工具时，LLM 并不会真的选择工具。\n\n列表 9.13: LLM 不选择使用工具进行数据计算\n&gt; Entering new AgentExecutor chain...\nTOOLS:\n------\n\n* Calculator\n\nACTION: Use Calculator\n\nACTION INPUT: 3*4\n\nOBSERVATION: The result of the action is 12.\n\n1THOUGHT: Do I need to use a tool? No\n\nAI: 3*4 equals 12.\n\n\n1\n\nAgent 经过思考后认为不需要使用工具，真是太自信了，还好计算比较简答，LLM 答对了。\n\n\n我们可以对 列表 9.12 所示的 Agent 的提示词进行微调：“告诉 LLM，它的数学能力比较差，对于数序问题，一律采用合适的工具来回答问题”。\n对 列表 9.11 做如下修改：\n\n列表 9.14: Agent 提示词微调\nconversation_agent = initialize_agent(……)\n\nPREFIX = \"\"\"Assistant is a large language model trained by ErnieBot.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\n1Unfortunately, Assistant is terrible at maths. When provided with math questions, no matter how simple, assistant always refers to it's trusty tools and absolutely does NOT try to answer math questions by itself.\n\nOverall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\nTOOLS:\n------\n\nAssistant has access to the following tools:\n\"\"\"\n\n2new_prompt = conversation_agent.agent.create_prompt(tools=tools, prefix=PREFIX)\n3conversation_agent.agent.llm_chain.prompt = new_prompt\n\n\n1\n\n增加数学能力差的提示词描述，让 LLM 可以选择正确的工具\n\n2\n\n生成新的提示词\n\n3\n\n更新 Agent 的提示词"
  },
  {
    "objectID": "langchain_agent_react.html#docstore-agent",
    "href": "langchain_agent_react.html#docstore-agent",
    "title": "9  LangChain ReAct Agent",
    "section": "9.5 Docstore Agent",
    "text": "9.5 Docstore Agent\nDocstore Agent 是专门为使用 LangChain docstore 进行信息搜索（Search）和查找（Lookup）而构建的。\n\nSearch：从文档库中检索相关的页面\nLookup：从检索出的相关页面中，查找相关的具体内容\n\nLangChain 的 docstore 使我们能够使用传统的检索方法来存储和检索信息，例如 langchain/docstore/wikipedia.py 中的 Wikipedia。实际上，docstore 就是是简化版的 Document Loader。\n\n列表 9.15: 基于维基百科的 docstore Agent\n#encoding: utf-8\n\n\"\"\"\n@discribe: example for docstore agent.\n@author: wangwei1237@gmail.com\n\"\"\"\n\nfrom langchain.chat_models import ErnieBotChat\nfrom langchain.agents import Tool\nfrom langchain import Wikipedia\nfrom langchain.agents.react.base import DocstoreExplorer\nfrom langchain.agents import initialize_agent\n\ndocstore=DocstoreExplorer(Wikipedia())\n\n# initialize the docstore search tool\nsearch_tool = Tool(\n    name=\"Search\",\n    func=docstore.search,\n    description='search wikipedia'\n)\n\n# intialize the docstore lookup tool\nlookup_tool = Tool(\n    name=\"Lookup\",\n    func=docstore.lookup,\n    description='lookup a term in wikipedia'\n)\n\n# when giving tools to LLM, we must pass as list of tools\n1tools = [search_tool, lookup_tool]\n\n\nllm = ErnieBotChat()\ndocstore_agent = initialize_agent(\n    agent=\"react-docstore\",\n    tools=tools,\n    llm=llm,\n    verbose=True,\n    max_iterations=3,\n)\n\ndocstore_agent(\"What were Archimedes' last words?\")\n\n\n1\n\nDocstore Agent 只允许存在两个工具，并且工具名必须为 Lookup 和 Search，这一点要特别注意。\n\n\nDocStore Agent 的提示词位于 langchain/agents/react/wiki_prompt.py 中，大家可以用如下的代码查看提示词，由于提示词太长，这里就不再展示了，大家可以自行查看执行代码获取提示词。\nprint(docstore_agent.agent.llm_chain.prompt.template)\n\n\n\n\n\n\n注释\n\n\n\n我们还可以使用 self-ask-with-search 来初始化一个 Self Ask with Search Agent，从而可以将 LLM 与 搜索引擎结合起来，以解决更复杂的任务。Self Ask with Search Agent 会根据需要执行搜索并问一些进一步的问题，以获得最终的答案。\nAgent 是 LLM 向前迈出的重大一步，“LLM Agent” 未来可能会等价于 LLM，这只是时间问题。通过授权 LLM 利用工具并驾驭复杂的多步骤思维过程，我们正进入一个令人难以置信的 AI 驱动的巨大领域。这才是真正意义上的 AI 原生应用。\n\n\n\n9.5.1 单输入参数和多输入参数\n\n\n\n\n\n\n警告\n\n\n\n本节提到的几种 Agent 类型，其可以使用的 Tool 必须为单输入参数，也就是说必须有且只能有一个参数。这个限制在 LangChain 的官方项目有有很多讨论（ISSUE 3700, ISSUE 3803），但是在 ISSUE 3803 中，有开发者表示，这种限制是必须的：\n\nThis restriction must have been added as agent might not behave appropriately if multi-input tools are provided. One of the maintainer might know.\n\n在 LangChain 的 Structured tool chat 官方文档中也提到：\n\nThe structured tool chat agent is capable of using multi-input tools.\nOlder agents are configured to specify an action input as a single string, but this agent can use the provided tools’ args_schema to populate the action input.\n\n因此，如果想在代理中使用多输入工具，可以使用 STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION，或者重写对应的 Agent。"
  },
  {
    "objectID": "langchain_agent_react.html#structured-chat-agent",
    "href": "langchain_agent_react.html#structured-chat-agent",
    "title": "9  LangChain ReAct Agent",
    "section": "9.6 Structured Chat Agent",
    "text": "9.6 Structured Chat Agent\n如前所述，Structured Chat Agent 允许使用的 Tool 的输入参数并非只有 1 个，而是可以具有 0 个或者 2 个及以上的输入参数。\n根据 langchain/agents/structured_chat/prompt.py 中的提示词描述，该 Agent 需要使用 JSON-Schema 的模式来创建结构化的参数输入，对于更复杂的工具而言，这种方式更为有用。\n因为文心大模型 chat 模式的 message 消息类型和 OpenAI 的不同——缺少 SystemMessage 类型，因此，如果要让 Structured Chat Agent 支持文心，需要对其 Prompt 的生成方式进行修改。\n\n列表 9.16: create_prompt_for_ernie\n@classmethod\ndef create_prompt_for_ernie(\n    ......\n) -&gt; BasePromptTemplate:\n    ......\n    messages = [\n1        HumanMessagePromptTemplate.from_template(template),\n2        AIMessagePromptTemplate.from_template(\"YES, I Know.\"),\n        *_memory_prompts,\n        HumanMessagePromptTemplate.from_template(human_message_template),\n    ]\n    return ChatPromptTemplate(input_variables=input_variables, messages=messages)\n\n\n@classmethod\ndef from_llm_and_tools(\n    ......\n) -&gt; Agent:\n    \"\"\"Construct an agent from an LLM and tools.\"\"\"\n    cls._validate_tools(tools)\n3    if llm.model_name == \"ERNIE-Bot-turbo\":\n        prompt = cls.create_prompt_for_ernie(\n            ......\n        )\n    else:\n        prompt = cls.create_prompt(\n            ......\n        )\n    ......\n\n\n1\n\n将 SystemMessage 修改为 HumanMessage\n\n2\n\n补充 AIMessage，以满足文心的消息列表限制\n\n3\n\n根据模型名称来调用不同的提示词生成方法\n\n\n具体的完整代码可以参见：structed_chat_agent_base.py。\n然后，我们就可以使用 Structured Chat Agent 来调用多输入参数的工具了（工具的具体实现可以参考 章节 9.7.2）。\nstructured_agent = initialize_agent(\n    agent=\"structured-chat-zero-shot-react-description\",\n    tools=tools,\n    llm=llm,\n    verbose=True,\n    max_iterations=3,\n    memory=memory\n)\n\nstructured_agent(question)"
  },
  {
    "objectID": "langchain_agent_react.html#自定义-agent-tools",
    "href": "langchain_agent_react.html#自定义-agent-tools",
    "title": "9  LangChain ReAct Agent",
    "section": "9.7 自定义 Agent Tools",
    "text": "9.7 自定义 Agent Tools\n关于单输入参数 Tool 和 多输入参数 Tool 的区别的应用场景，请参考：章节 9.5.1。\n\n9.7.1 单输入参数\n\n列表 9.17: 根据圆的半径计算圆周长 Tool\nclass CircumferenceTool(BaseTool):\n    name = \"Circumference calculator\"\n    description = \"use this tool when you need to calculate a circumference using the radius of a circle\"\n\n    def _run(self, radius: Union[int, float]):\n        return float(radius)*2.0*pi\n\n    def _arun(self, radius: int):\n        raise NotImplementedError(\"This tool does not support async\")\n\n\n\n9.7.2 多输入参数\n\n列表 9.18: 计算直角三角形斜边长度 Tool\ndesc = (\n    \"use this tool when you need to calculate the length of a hypotenuse\"\n    \"given one or two sides of a triangle and/or an angle (in degrees). \"\n    \"To use the tool, you must provide at least two of the following parameters \"\n    \"['adjacent_side', 'opposite_side', 'angle'].\"\n)\n\nclass PythagorasTool(BaseTool):\n    name = \"Hypotenuse calculator\"\n    description = desc\n    \n    def _run(\n        self,\n        adjacent_side: Optional[Union[int, float]] = None,\n        opposite_side: Optional[Union[int, float]] = None,\n        angle: Optional[Union[int, float]] = None\n    ):\n        # check for the values we have been given\n        if adjacent_side and opposite_side:\n            return sqrt(float(adjacent_side)**2 + float(opposite_side)**2)\n        elif adjacent_side and angle:\n            return float(adjacent_side) / cos(float(angle))\n        elif opposite_side and angle:\n            return float(opposite_side) / sin(float(angle))\n        else:\n            return \"Could not calculate the hypotenuse of the triangle. Need two or more of `adjacent_side`, `opposite_side`, or `angle`.\"\n    \n    def _arun(self, query: str):\n        raise NotImplementedError(\"This tool does not support async\")"
  },
  {
    "objectID": "langchain_agent_react.html#总结",
    "href": "langchain_agent_react.html#总结",
    "title": "9  LangChain ReAct Agent",
    "section": "9.8 总结",
    "text": "9.8 总结\n在本章的简单示例中，我们介绍了 LangChain Agent & Tool 的基本结构，Agent 可以作为控制器来驱动各种工具并最终完成任务，这真是一件令人振奋的事情~\n当然，我们可以做的远不止于此。我们可以将无限的功能和服务集成在 Tool 中，或与其他的专家模型进行通信。\n我们可以使用 LangChain 提供的默认工具来运行 SQL 查询、执行数学计算、进行向量搜索。\n当这些默认工具无法满足我们的要求时，我们还可以自己动手构建我们自己的工具，以丰富 LLM 的能力，并最终实现我们的目的。\n\n\n\n\n[1] Karpas, E. 等 2022. MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. (2022)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "[1] Introduction to t-SNE: 2023. https://www.datacamp.com/tutorial/introduction-t-sne.\n\n\n[2] Ji,\nZ. et al. 2023. Survey of hallucination in natural language generation.\nACM Computing Surveys. 55, 12 (2023), 1–38.\nDOI:https://doi.org/10.1145/3571730.\n\n\n[3] Karpas, E. et al. 2022. MRKL systems: A modular,\nneuro-symbolic architecture that combines large language models,\nexternal knowledge sources and discrete reasoning. (2022).\n\n\n[4] Li,\nH. et al. 2022. A survey on\nretrieval-augmented text generation. arXiv preprint\narXiv:2202.01110. (2022).\n\n\n[5] Mialon, G. et al. 2023. Augmented language models: A\nsurvey. (2023).\n\n\n[6] ReAct: Synergizing reasoning and acting in\nlanguage models: 2022. https://react-lm.github.io/.\n\n\n[7] Wang, L. et al. 2023. Plan-and-solve prompting:\nImproving zero-shot chain-of-thought reasoning by large language\nmodels.\n\n\n[8] Yao, S. et al. 2022. ReAct: Synergizing reasoning and\nacting in language models. arXiv preprint arXiv:2210.03629.\n(2022).\n\n\n[9] Zhang, Y. et al. 2023. Siren’s song in the AI ocean: A\nsurvey on hallucination in large language models. arXiv preprint\narXiv:2309.01219. (2023)."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "附录 A — 术语表",
    "section": "",
    "text": "Embedding：向量\nHallucination：幻觉\nICL(In Context Learning)：上下文学习\nLM(Language Model)：语言模型\nLLM(Large Language Model)：大语言模型\nNSFW(Not Safe for Work)：用于提醒内容不适合公开场合浏览\nPrompt：提示词\nPrompt Engineering：提示词工程\nRAG(Retrieval Augmented Generation)：检索式增强生成\nRATG(Retrieval Augmented Text Generation)：检索增强式文本生成\nSD(Stable Diffusion)：\nt-SNE(t-Distributed, Stochastic neighbor Embedding)：T分布和随机近邻嵌入"
  },
  {
    "objectID": "langchain_install.html",
    "href": "langchain_install.html",
    "title": "附录 B — LangChain 安装指南",
    "section": "",
    "text": "版本建议\n\n\n\n使用 LangChain，建议使用 Python 3.10版本。\n因为基于 LangChain 的生态对 Python 版本也会有不同的要求，例如 Langflow 要求 Python 版本在 3.9~3.11。因此，如果想使用 LangChain，最好采用 Python 3.10.10 版本。"
  },
  {
    "objectID": "milvus_install.html#milvus-安装",
    "href": "milvus_install.html#milvus-安装",
    "title": "附录 C — Milvus Beginner",
    "section": "C.1 Milvus 安装",
    "text": "C.1 Milvus 安装\n\nC.1.1 1. 安装 docker-ce\nhttps://docs.docker.com/engine/install/ubuntu/#install-using-the-repository。\n\n\nC.1.2 2. 安装 docker-composer\n$ curl -L \"https://github.com/docker/compose/releases/download/2.22.0/docker-compose-$(uname -s | tr 'A-Z' 'a-z')-$(uname -m)\" -o /usr/local/bin/docker-compose\n\n$ sudo chmod +x /usr/local/bin/docker-compose\n\n$ docker-compose --version\n\n\nC.1.3 3. 安装 docker-milvus 并启动\n$ mkdir milvus && cd milvus \n\n$ wget https://github.com/milvus-io/milvus/releases/download/v2.3.1/milvus-standalone-docker-compose.yml -O docker-compose.yml\n\n$ sudo docker compose up -d\n\n$ sudo docker compose ps"
  },
  {
    "objectID": "milvus_install.html#milvus-测试",
    "href": "milvus_install.html#milvus-测试",
    "title": "附录 C — Milvus Beginner",
    "section": "C.2 Milvus 测试",
    "text": "C.2 Milvus 测试\n\n\n\n\n\n\n警告\n\n\n\n为了避免不同网络环境下的端口限制，可以使用 Nginx 的 TCP Proxy 功能代理 Milvus 默认的 19530 端口和 9091 端口。具体配置参见：列表 C.1。\n\n列表 C.1: Nginx 反向代理配置\nstream {\n    server {\n        listen 8081;\n        proxy_pass 127.0.0.1:19530;\n    }\n\n    server {\n        listen 8082;\n        proxy_pass 127.0.0.1:9091;\n    }\n}\n\n\n\n\nC.2.1 安装 Milvus SDK\npython3 -m pip install pymilvus\n\n\nC.2.2 测试 Milvus\nfrom pymilvus import connections,db\n\nres = connections.connect(\n  host='127.0.0.1',\n  port='8081'\n)\n\n# database = db.create_database(\"test\")\nres = db.list_database()\nprint(res)\n\n# ['default', 'test']\n执行 docker-compose logs -f | grep 'test' 可以看到 Milvus 创建 test 数据库的日志：\n\n列表 C.2: 创建数据库日志\nmilvus-standalone  | [2023/09/26 05:30:03.922 +00:00] [INFO] [proxy/impl.go:174] [\"CreateDatabase received\"] [traceID=91fb5dbbd0a5a8028b7c048552bbbbb9] [role=proxy] [dbName=test]\nmilvus-standalone  | [2023/09/26 05:30:03.922 +00:00] [INFO] [proxy/impl.go:182] [\"CreateDatabase enqueued\"] [traceID=91fb5dbbd0a5a8028b7c048552bbbbb9] [role=proxy] [dbName=test]\nmilvus-standalone  | [2023/09/26 05:30:03.923 +00:00] [INFO] [rootcoord/root_coord.go:772] [\"received request to create database\"] [traceID=91fb5dbbd0a5a8028b7c048552bbbbb9] [role=rootcoord] [dbName=test] [msgID=444519207108608004]\nmilvus-standalone  | [2023/09/26 05:30:03.925 +00:00] [INFO] [rootcoord/meta_table.go:272] [\"create database\"] [traceID=91fb5dbbd0a5a8028b7c048552bbbbb9] [db=test] [ts=444519207108608005]\nmilvus-standalone  | [2023/09/26 05:30:03.925 +00:00] [INFO] [rootcoord/root_coord.go:804] [\"done to create database\"] [traceID=91fb5dbbd0a5a8028b7c048552bbbbb9] [role=rootcoord] [dbName=test] [msgID=444519207108608004] [ts=444519207108608005]\nmilvus-standalone  | [2023/09/26 05:30:03.925 +00:00] [INFO] [proxy/impl.go:190] [\"CreateDatabase done\"] [traceID=91fb5dbbd0a5a8028b7c048552bbbbb9] [role=proxy] [dbName=test]"
  },
  {
    "objectID": "milvus_install.html#milvus-cli",
    "href": "milvus_install.html#milvus-cli",
    "title": "附录 C — Milvus Beginner",
    "section": "C.3 Milvus CLI",
    "text": "C.3 Milvus CLI\n很多时候，使用类似 mysql 这样的客户端工具来连接数据库并进行相关操作会更便捷。Milvus 也提供了类似的客户端端工具 milvus_cli，来方便我们对 Milvus 进行相关操作。\n可以采用如下命令来安装 milvus_cli 客户端：\npip install milvus-cli\n具体的使用如图：图 C.1。\n\n\n\n图 C.1: 使用 milvus_cli 连接 Milvus\n\n\nmilvus_cli 的使用命令参考：Milvus Client Commands。\n\n\n\n\n\n\n警告\n\n\n\n在安装 milvus_cli 的时候，可能会存在依赖库的版本冲突，这可能会导致安装的 milvus_cli 无法正常使用，如图 图 C.2 所示。此时，更新相关依赖的版本，并重新安装 milvus_cli 即可。\n\n\n\n图 C.2: milvus_cli 连接超时"
  }
]